# Toshik Babe Engine — Документ требований к продукту (PRD)

**Версия:** 1.1
**Дата:** 8 февраля 2026
**Автор:** Senior Product Manager
**Статус:** Черновик для ревью

---

## 1. Резюме проекта

**Toshik Babe Engine** — суверенная система ИИ-ассистента, построенная на принципе local-first. Продукт представляет собой быструю, глубоко интегрированную и приватную альтернативу облачным решениям класса OpenAI/ChatGPT, где пользователь полностью владеет своими данными, а вычисления максимально приближены к локальной машине или личному серверу.

**Технологический стек:**
- Бэкенд: Bun (JavaScript-рантайм с нативной производительностью)
- Фронтенд: Tauri 2.x + React (нативное десктопное приложение с системным WebView)
- Хранение: SQLite (структурированные данные) + векторная БД (семантический поиск)

**Режимы развёртывания:**
- **Локальный** — Bun-бэкенд запускается на той же машине, что и Tauri-клиент. Полностью автономная работа без выхода в интернет (при использовании локальных моделей).
- **Клиент-серверный** — Bun-бэкенд работает на удалённом VPS пользователя (например, бюджетный хостинг типа Beget, 1–2 vCPU, 1–2 ГБ RAM). Tauri-клиент подключается по WebSocket через интернет. Все данные остаются на контролируемом пользователем сервере — никакие третьи стороны не участвуют в обработке.

---

## 2. Цели продукта

### 2.1 Проблема

Существующие ИИ-ассистенты страдают от трёх фундаментальных ограничений:

**Приватность.** Все данные пользователя — переписки, файлы, контекст — уходят на серверы третьих сторон. Пользователь не контролирует, как его данные хранятся, обрабатываются и монетизируются. Для профессионалов, работающих с конфиденциальной информацией (юристы, врачи, разработчики с проприетарным кодом), это неприемлемо.

**Скорость и отзывчивость.** Облачные решения зависят от сетевых задержек. Каждое действие — отправка сообщения, переключение контекста, загрузка истории — требует round-trip до сервера. UI зачастую реактивный, а не проактивный.

**Жёсткость интеграций.** Пользователь заперт в экосистеме провайдера. Невозможно подключить произвольную модель, настроить пайплайн обработки под свои задачи, интегрировать локальные инструменты и базы знаний без потери удобства. Взаимодействие ограничено «песочницей» браузера и не имеет доступа к операционной системе.

### 2.2 Решение

Toshik Babe Engine решает эти проблемы через три ключевых принципа:

- **Суверенность данных:** все данные хранятся локально или на VPS пользователя. Полный отказ от телеметрии третьих сторон. Пользователь единолично решает, что и когда покидает его устройство.
- **Нативная скорость:** Bun как бэкенд + Tauri как оболочка дают производительность, сопоставимую с нативными приложениями, при минимальном потреблении ресурсов — включая работу на бюджетных VPS.
- **Глубокая интеграция:** модульная архитектура позволяет подключать любые LLM-провайдеры, а Tauri обеспечивает прямой доступ к ОС — буферу обмена, активному окну, файловой системе — выходя за рамки браузерной песочницы.

### 2.3 Ключевые метрики успеха (KPI)

| Метрика | Целевое значение | Срок |
|---|---|---|
| Время до первого токена ответа (UI) | < 200 мс | MVP |
| Потребление RAM бэкенда в простое | < 150 МБ | MVP |
| Размер установщика | < 50 МБ | MVP |
| Время холодного старта приложения | < 2 сек | MVP |
| Стабильная работа бэкенда на VPS 1 vCPU / 1 ГБ RAM | Подтверждена | MVP |
| Retention D7 (бета-тестеры) | > 60% | Бета |
| NPS (бета-тестеры) | > 50 | Бета |

---

## 3. Целевая аудитория

**Первичная:** Разработчики и технические специалисты, которые ежедневно работают с ИИ-ассистентами и ценят контроль над инструментами, приватность и возможность кастомизации.

**Вторичная:** Продвинутые пользователи из смежных областей (аналитики, исследователи, технические писатели), которым нужен мощный, но приватный ИИ-инструмент без привязки к облачным экосистемам.

**Антиаудитория (на текущем этапе):** Массовый нетехнический пользователь, который ожидает полностью облачный опыт «из коробки» без настроек.

---

## 4. Функциональные требования

### 4.1 Ядро коммуникации: WebSocket-слой

**Описание:** Вся коммуникация между фронтендом (Tauri/React) и бэкендом (Bun) осуществляется через постоянное WebSocket-соединение. Это обеспечивает мгновенную двустороннюю передачу данных без HTTP-оверхеда как в локальном режиме, так и при подключении к удалённому VPS.

**Требования:**

- FR-1.1: Установка WebSocket-соединения при старте приложения с автоматическим переподключением (exponential backoff, максимум 5 попыток).
- FR-1.2: Стриминг ответов от LLM token-by-token через WebSocket с поддержкой отмены генерации в реальном времени.
- FR-1.3: Типизированный протокол сообщений (JSON-схема): каждое сообщение имеет `type`, `payload`, `timestamp`, `correlation_id`.
- FR-1.4: Мультиплексирование каналов — одно соединение обслуживает чат, обновления виджетов, системные уведомления и синхронизацию состояния.
- FR-1.5: Heartbeat-механизм (ping/pong каждые 30 сек) для детектирования разрыва соединения.
- FR-1.6: Очередь сообщений на стороне клиента — при разрыве соединения сообщения буферизуются и отправляются после восстановления.
- FR-1.7: Менеджер сессий — поддержка persistent-соединений с возможностью полного восстановления контекста (активная беседа, состояние генерации, очередь) при обрыве связи.

### 4.2 Мультимодальная поддержка

**Описание:** Система обрабатывает не только текст, но и изображения, документы и файлы, формируя богатый контекст для ИИ.

**Требования:**

- FR-2.1: **Текст** — ввод сообщений с поддержкой Markdown-разметки, подсветки синтаксиса кода (основные языки), LaTeX-формул.
- FR-2.2: **Vision** — загрузка изображений (PNG, JPEG, WebP, GIF) через drag-and-drop, буфер обмена или диалог файлов. Поддержка скриншотов: глобальная горячая клавиша для захвата области экрана и мгновенной отправки в чат. Превью изображения в чате. Передача изображений в модель для анализа (если модель поддерживает vision).
- FR-2.3: **Файлы** — загрузка документов (PDF, TXT, MD, CSV, DOCX, файлы кода) с автоматической экстракцией текста и индексацией на стороне бэкенда. Отображение метаданных файла в чате (имя, размер, тип). Возможность ссылаться на загруженный файл в рамках беседы.
- FR-2.4: Ограничения: максимальный размер одного файла — 50 МБ, максимум 10 файлов за одно сообщение.
- FR-2.5: Прогресс-индикатор загрузки и обработки для каждого прикреплённого файла.

### 4.3 Система памяти Local-First

**Описание:** Двухуровневая система хранения, объединяющая структурированное хранилище (SQLite) и семантический поиск (векторный RAG), обеспечивает долговременную контекстную память ассистента. Все данные хранятся локально или на VPS пользователя.

**Требования:**

- FR-3.1: **SQLite-хранилище** — все беседы, сообщения, метаданные сессий, настройки пользователя, логи хранятся в локальной SQLite-базе.
- FR-3.2: **Векторный индекс** — при сохранении сообщений автоматически генерируются эмбеддинги (через локальную модель или API) и сохраняются в векторное хранилище (SQLite-vss, LanceDB или аналогичное легковесное решение).
- FR-3.3: **RAG-пайплайн** — перед отправкой запроса к LLM система автоматически ищет релевантный контекст из прошлых бесед и загруженных документов по семантической близости и инжектирует его в промпт.
- FR-3.4: Пользователь может управлять памятью: просматривать сохранённые факты, удалять отдельные записи, полностью очищать память.
- FR-3.5: Поддержка «баз знаний» — пользователь может загрузить набор документов, которые индексируются и доступны для RAG в рамках конкретного проекта или глобально.
- FR-3.6: Настраиваемый порог релевантности (cosine similarity threshold) для RAG-поиска.

### 4.4 Динамический UI: Двухзонная раскладка

**Описание:** Интерфейс разделён на две функциональные зоны, позволяющие одновременно вести диалог и видеть расширенный контекст.

**Требования:**

- FR-4.1: **Зона чата (левая/основная)** — классический интерфейс диалога: список сообщений, поле ввода, поддержка стриминга, Markdown-рендеринг, блоки кода с кнопкой копирования.
- FR-4.2: **Зона виджетов контекста (правая/боковая)** — динамическая панель, отображающая контекстную информацию в зависимости от текущего разговора:
  - Превью загруженных файлов и изображений.
  - Результаты RAG-поиска (релевантные фрагменты из памяти с указанием источника).
  - Предпросмотр ссылок, статус задач.
  - Метаданные текущей модели (имя, параметры, токены).
  - Артефакты: сгенерированный код, таблицы, диаграммы — вынесены в отдельные интерактивные карточки.
- FR-4.3: Зона виджетов сворачивается/разворачивается по клику или горячей клавишей. В свёрнутом состоянии чат занимает 100% ширины.
- FR-4.4: Виджеты подгружаются динамически по мере появления контента, не перегружая UI при отсутствии данных. Пустая правая панель не отображается.
- FR-4.5: Поддержка drag-and-drop для изменения порядка виджетов в правой панели.

### 4.5 Контекстная интеграция с ОС

**Описание:** Благодаря Tauri приложение имеет прямой доступ к операционной системе, что позволяет выйти за рамки браузерной песочницы и обеспечить глубокую контекстную осведомлённость.

**Требования:**

- FR-5.1: Через Tauri API приложение может читать **имя и заголовок активного окна** для автоматического определения рабочего контекста пользователя.
- FR-5.2: Настраиваемая глобальная горячая клавиша для **захвата выделенного текста** из любого приложения и отправки его в Toshik Babe Engine как контекста или запроса.
- FR-5.3: Горячая клавиша для **мгновенной вставки содержимого буфера обмена** в текущий промпт или как вложения (текст, изображение).
- FR-5.4: На основе полученного системного контекста приложение может **проактивно предлагать действия** (например: «Вижу, что вы работаете в VS Code с файлом `main.rs` — хотите проанализировать код?»).
- FR-5.5: Глобальная горячая клавиша для вызова мини-окна ассистента (Quick Input) поверх любого приложения — быстрый вопрос без переключения контекста.

### 4.6 Библиотека шаблонов промптов и Персоны

**Описание:** Двухуровневая система управления промптами: шаблоны для типовых задач и персоны для ролевого поведения ассистента.

**Требования:**

**Шаблоны промптов:**

- FR-6.1: Встроенный каталог шаблонов с категориями (Разработка, Тексты, Анализ, Переводы, Кастомные).
- FR-6.2: Каждый шаблон содержит: название, описание, тело промпта с переменными-плейсхолдерами, категорию, теги.
- FR-6.3: Быстрый доступ через символ `/` в поле ввода — открывает выпадающий поиск по шаблонам.
- FR-6.4: Пользователь может создавать, редактировать, удалять и экспортировать/импортировать собственные шаблоны (формат JSON).
- FR-6.5: При выборе шаблона с переменными отображается мини-форма для заполнения значений перед отправкой.
- FR-6.6: Поставляется набор из 20+ предустановленных шаблонов для типовых задач.
- FR-6.7: **Системные переменные** — шаблоны поддерживают встроенные переменные, автоматически заполняемые из системного контекста:
  - `{{selection}}` — текущий выделенный текст в ОС
  - `{{clipboard}}` — содержимое буфера обмена
  - `{{active_window}}` — имя/заголовок активного окна
  - `{{current_date}}`, `{{current_time}}` — дата и время
  - `{{file_path}}` — путь к текущему файлу (если определим)
  - Пользователь может добавлять собственные переменные.

**Персоны:**

- FR-6.8: **Персона** — отдельная сущность, определяющая роль и поведение ассистента. Включает:
  - Системный промпт (характер, роль, стиль общения)
  - Набор доступных инструментов / function calls
  - Привязанную модель (опционально — можно задать конкретную модель для персоны)
  - Привязанную базу знаний (опционально — персона может иметь доступ к определённому набору документов)
- FR-6.9: Пользователь может переключать персону «на лету» через UI или горячую клавишу, не прерывая беседу.
- FR-6.10: Поставляется набор предустановленных персон (Кодер, Писатель, Аналитик, Переводчик). Пользователь может создавать собственные.
- FR-6.11: Экспорт/импорт персон в формате JSON для обмена с сообществом.

### 4.7 Управление моделями и провайдерами

**Требования:**

- FR-7.1: Поддержка множественных LLM-провайдеров: OpenAI API, Anthropic API, Google AI, Ollama (локальные модели), LM Studio, Llama.cpp, любой OpenAI-совместимый эндпоинт.
- FR-7.2: UI для добавления/удаления/переключения провайдеров и моделей. Настройка API-ключей, base URL, параметров (temperature, max_tokens, top_p и т.д.).
- FR-7.3: Переключение модели «на лету» в рамках одной беседы.
- FR-7.4: Сохранение API-ключей в зашифрованном виде в локальном хранилище (OS Keychain через Tauri Stronghold или аналог).

### 4.8 Управление беседами

**Требования:**

- FR-8.1: Создание, переименование, удаление, архивирование бесед.
- FR-8.2: Папки/теги для организации бесед.
- FR-8.3: Полнотекстовый поиск по истории всех бесед.
- FR-8.4: Экспорт беседы в Markdown, JSON, PDF.
- FR-8.5: Форк беседы — создание ответвления с определённого сообщения.

---

## 5. Нефункциональные требования

### 5.1 Производительность и задержки

| Операция | Целевая задержка | Приоритет |
|---|---|---|
| Отклик UI на действие пользователя (клик, ввод) | < 50 мс | Критический |
| Отображение первого токена стриминга (Time to First Byte / отрисовка) | < 100 мс после получения от бэкенда | Критический |
| Открытие беседы из списка (до 10 000 сообщений) | < 200 мс | Высокий |
| Полнотекстовый поиск по 100 000 сообщений | < 500 мс | Высокий |
| RAG-поиск по векторной базе | < 300 мс | Высокий |
| Холодный старт приложения | < 2 сек | Высокий |
| Горячий рестарт WebSocket | < 500 мс | Средний |

**Ресурсоэффективность на бюджетных VPS:**
Бэкенд на Bun должен стабильно работать на серверах с 1 vCPU и 1 ГБ RAM (типовой бюджетный тариф). Это означает минимальный overhead по памяти и отсутствие CPU-интенсивных фоновых процессов в состоянии покоя.

### 5.2 Безопасность

- NFR-2.1: **Локальное владение данными и нулевая телеметрия** — все пользовательские данные (беседы, файлы, эмбеддинги, настройки) хранятся исключительно на устройстве пользователя или его VPS. Приложение не включает аналитические SDK третьих сторон (Google Analytics, Mixpanel, Sentry и т.д.), не загружает внешние шрифты, скрипты или ресурсы с CDN третьих сторон. Все ассеты бандлятся локально. Единственные исходящие сетевые запросы — к LLM-провайдерам, выбранным пользователем.
- NFR-2.2: **Шифрование базы данных** — SQLite-база шифруется через SQLCipher или аналог с ключом, производным от мастер-пароля пользователя.
- NFR-2.3: **E2E-шифрование синхронизации** — при связи клиента с удалённым VPS и при опциональной синхронизации между устройствами все данные шифруются end-to-end. Промежуточные узлы видят только зашифрованные блобы.
- NFR-2.4: **Безопасное хранение секретов** — API-ключи хранятся через OS Keychain (macOS Keychain, Windows Credential Manager, Linux Secret Service) через Tauri Stronghold или аналог.
- NFR-2.5: **Сетевая прозрачность** — пользователь видит лог всех исходящих сетевых запросов (к каким API, когда, какой объём данных). Никаких скрытых обращений.
- NFR-2.6: **Автообновления с подписью** — обновления приложения подписываются ключом разработчика и верифицируются перед установкой.

### 5.3 Эффективность ресурсов

| Ресурс | Лимит (простой) | Лимит (активная работа) |
|---|---|---|
| RAM (бэкенд, Bun) | < 150 МБ | < 400 МБ |
| RAM (клиент, Tauri) | < 100 МБ | < 250 МБ |
| CPU | < 1% | < 15% (без учёта локальных моделей) |
| Дисковое пространство (приложение) | < 50 МБ | — |
| Дисковое пространство (данные) | Зависит от пользователя | Предупреждение при > 5 ГБ |

- NFR-3.1: Отсутствие Electron — Tauri использует системный WebView, что радикально снижает потребление RAM.
- NFR-3.2: Ленивая загрузка — компоненты UI и данные подгружаются по требованию, а не при старте.
- NFR-3.3: Виртуализация списков — длинные списки сообщений и бесед рендерятся через виртуализацию (react-virtuoso или аналог).

### 5.4 Совместимость

- NFR-4.1: Поддерживаемые ОС: macOS 12+, Windows 10+, Linux (Ubuntu 22.04+, Fedora 38+).
- NFR-4.2: Минимальные системные требования: 4 ГБ RAM, 500 МБ свободного места, интернет-подключение (для облачных моделей) или локальная модель.
- NFR-4.3: Бэкенд (Bun) совместим с бюджетными VPS: 1 vCPU, 1 ГБ RAM, Ubuntu 22.04+.

### 5.5 Надёжность

- NFR-5.1: Автосохранение текущего ввода каждые 5 секунд — пользователь не теряет набранный текст при сбое.
- NFR-5.2: **Graceful degradation** — при отсутствии сети приложение продолжает работать: доступны локальная история, поиск, шаблоны, персоны. Отображается статус подключения.
- NFR-5.3: Автоматические бэкапы базы данных (ежедневно, ротация 7 дней).

---

## 6. Принципы пользовательского опыта (UX)

### 6.1 Минимализм

Интерфейс должен содержать только то, что нужно прямо сейчас. Никаких кнопок «на всякий случай», никаких вложенных меню ради полноты. Каждый элемент на экране должен заслужить своё место. Вдохновение: iA Writer (фокус на содержимом), Linear (чистота и скорость), Arc (смелость UX-решений).

### 6.2 Нулевое отвлечение (Zero-Distraction)

- Нет анимаций ради анимаций — переходы только для ориентации пользователя (< 200 мс).
- Нет push-уведомлений, баннеров или промо-элементов внутри приложения.
- Уведомления и виджеты появляются только тогда, когда они действительно нужны.
- Режим **«Focus Mode»** — скрывает боковую панель и список бесед, оставляя только чат.
- Тёмная и светлая тема с возможностью следовать системным настройкам.

### 6.3 Контекстная осведомлённость

Интерфейс адаптируется к тому, что делает пользователь — как внутри приложения, так и за его пределами:

- Если в ответе есть код — автоматически появляется виджет с подсветкой и кнопкой копирования.
- Если загружен файл — правая панель показывает его превью и метаданные.
- Если RAG нашёл релевантный контекст — отображается виджет «Из памяти» с источниками.
- Пустая правая панель не отображается — она появляется только когда есть что показать.
- Система понимает, **над чем работает пользователь в ОС** (активное окно, выделенный текст), и может предлагать релевантные действия проактивно.

### 6.4 Клавиатуро-центричность

- Все ключевые действия доступны через горячие клавиши (Cmd/Ctrl + K для Command Palette).
- Навигация между беседами, моделями, шаблонами, персонами — без мыши.
- Slash-команды (`/`) для быстрого доступа к шаблонам и действиям.
- Глобальные горячие клавиши для вызова приложения, захвата выделенного текста и скриншотов.

### 6.5 Прогрессивное раскрытие сложности

- По умолчанию: простой интерфейс с одной моделью и чатом.
- Для продвинутых: настройки параметров модели, RAG-тюнинг, множественные провайдеры, персоны, системные переменные — доступны, но спрятаны до востребования.

---

## 7. Перспективы и Future-Proofing

### 7.1 Агностицизм моделей (Model Agnostic Layer)

Ландшафт LLM меняется каждые 3–6 месяцев. Архитектура должна быть к этому готова:

- **Абстракция провайдера** — унифицированный интерфейс `ModelProvider` с методами `chat()`, `stream()`, `embed()`. Новый провайдер = новый класс, реализующий интерфейс, без изменений в UI или бизнес-логике.
- **Динамическая конфигурация моделей** — параметры модели (context window, vision support, function calling) описываются в конфиге, а не хардкодятся. При появлении модели с окном в 1М+ токенов или модели нового поколения (агентные, с бесконечным контекстом) — достаточно обновить конфиг.
- **Поддержка function calling / tool use** — архитектура готова к тому, что модель может вызывать инструменты (поиск, калькулятор, выполнение кода).
- **Горячая замена** — переключение между провайдерами (OpenAI, Anthropic, Google, Ollama, Llama.cpp) без перезапуска приложения.

### 7.2 Автономные агенты и фоновые sub-agents (Phase 2–3)

- Закладка фундамента для запуска **фоновых sub-agents**, способных выполнять долгосрочные задачи без активного окна чата.
- Примеры сценариев: мониторинг RSS-ленты и подготовка саммари, периодический анализ кодовой базы, ежедневная компиляция дайджеста, длительный ресёрч.
- Архитектурные требования: очередь задач в бэкенде, lifecycle-менеджмент агентов (запуск, пауза, остановка, отчёт), UI для просмотра статуса и результатов фоновых задач.
- Оркестрация сценариев, где несколько агентов кооперируются (исследователь → кодер → ревьюер), с визуализацией цепочки в UI.

### 7.3 Плагинная архитектура (Phase 2)

- Стандартизированный Plugin API для расширения функциональности.
- Плагины пишутся на **JavaScript или TypeScript** и исполняются в **изолированной среде Bun** (worker threads или аналогичный sandbox). Единый рантайм для плагинов и основного бэкенда упрощает экосистему и снижает порог входа для разработчиков.
- Плагины могут добавлять: новые виджеты в правую панель, новые типы обработки файлов, новые инструменты для function calling, интеграции с внешними сервисами (Notion, Jira, GitHub).
- Плагины работают в песочнице с явными разрешениями (доступ к сети, файловой системе, API-ключам — только с согласия пользователя).

### 7.4 Синхронизация между устройствами (Phase 2)

- Опциональная E2E-зашифрованная синхронизация через self-hosted сервер или P2P.
- CRDT-подход для бесконфликтного слияния изменений.

### 7.5 Мобильный компаньон (Phase 3)

- Tauri 2.x поддерживает мобильные платформы — возможность создания iOS/Android-клиента на том же стеке.

---

## 8. Дорожная карта (высокий уровень)

| Фаза | Срок | Объём |
|---|---|---|
| **Phase 0 — Прототип** | 4 недели | WebSocket-ядро, базовый чат, 1 провайдер (OpenAI), SQLite-хранение, локальный режим |
| **Phase 1 — MVP** | 8 недель | Мультипровайдер, Vision, файлы, RAG-память, двухзонный UI, шаблоны с системными переменными, персоны, интеграция с ОС, режим VPS |
| **Phase 2 — Расширение** | 12 недель | Плагин-система (JS/TS на Bun), синхронизация между устройствами, продвинутый RAG, function calling, фоновые агенты (базовый) |
| **Phase 3 — Масштаб** | 16+ недель | Мультиагентность и оркестрация, мобильный клиент, маркетплейс плагинов |

---

## 9. Риски и митигации

| Риск | Вероятность | Влияние | Митигация |
|---|---|---|---|
| Нестабильность Tauri 2.x на отдельных ОС | Средняя | Высокое | Ранний кросс-платформенный CI/CD, тестирование на всех ОС с Phase 0 |
| Быстрое устаревание LLM API-контрактов | Высокая | Среднее | Абстракция провайдера, семантическое версионирование конфигов |
| Низкое качество локальных эмбеддингов для RAG | Средняя | Среднее | Поддержка как локальных, так и облачных эмбеддинг-моделей; A/B-тестирование качества |
| Сложность для нетехнических пользователей | Высокая | Среднее | Прогрессивное раскрытие, wizard первого запуска, пресеты |
| Ограничения ресурсов бюджетных VPS | Средняя | Высокое | Профилирование на минимальных конфигурациях с Phase 0, стресс-тесты |
| Ограничения доступа к ОС на разных платформах | Средняя | Среднее | Graceful degradation: функции интеграции с ОС опциональны, приложение работает и без них |

---

## 10. Критерии приёмки MVP

MVP считается готовым, когда выполнены все условия:

1. Пользователь может установить приложение на macOS, Windows или Linux за < 2 минут.
2. Приложение работает как в локальном режиме, так и с удалённым Bun-бэкендом на VPS.
3. Пользователь может подключить минимум 2 LLM-провайдера и переключаться между ними.
4. Чат работает в режиме стриминга с задержкой отображения первого токена < 100 мс.
5. Пользователь может загрузить изображение и документ, и модель учитывает их в ответе.
6. Система автоматически находит и подставляет релевантный контекст из прошлых бесед (RAG).
7. Двухзонный UI отображает контекстные виджеты без ручного управления.
8. Библиотека шаблонов содержит 20+ предустановленных, поддерживает системные переменные (`{{selection}}`, `{{clipboard}}` и т.д.) и пользовательские шаблоны.
9. Доступны минимум 3 предустановленные персоны, пользователь может создавать свои.
10. Глобальная горячая клавиша для захвата выделенного текста и вызова Quick Input работает на всех поддерживаемых ОС.
11. Все данные хранятся локально/на VPS, API-ключи — в зашифрованном хранилище ОС.
12. Приложение не совершает никаких сетевых запросов к третьим сторонам, кроме выбранных пользователем LLM-провайдеров.
13. Потребление RAM бэкенда в простое < 150 МБ, холодный старт < 2 сек.
14. Бэкенд стабильно работает на VPS с 1 vCPU и 1 ГБ RAM.

---

## 11. Открытые вопросы

1. **Выбор векторного хранилища** — SQLite-vss, LanceDB или встраиваемый HNSW (hnswlib)? Требуется бенчмарк, в том числе на бюджетных VPS.
2. **Локальные эмбеддинги** — какой минимальный размер модели даёт приемлемое качество для RAG на потребительском оборудовании и VPS?
3. **Лицензирование** — open-source (MIT/Apache 2.0) или source-available с коммерческой лицензией?
4. **Монетизация** — freemium с pro-функциями, одноразовая покупка или полностью бесплатный OSS?
5. **Границы доступа к ОС** — какие функции интеграции с ОС (активное окно, выделенный текст) доступны на каждой платформе? Требуется аудит Tauri API.
6. **Песочница плагинов** — какой уровень изоляции обеспечивает Bun worker threads? Достаточно ли для безопасного исполнения стороннего кода?

---

*Документ подлежит ревью всей командой. Комментарии и предложения направлять через Issue Tracker проекта.*